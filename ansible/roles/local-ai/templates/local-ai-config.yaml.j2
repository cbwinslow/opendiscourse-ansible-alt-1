# LocalAI configuration
debug: false
threads: 4
models: {{ local_ai_base_dir }}/models
preload_models: []
context_size: 512
parallel_requests: false
cors: true
cors_allow_origins: []
cuda: false
backend: llama-stable
f16: false
debug: false
load_configs: []
max_tokens: 512
model: {{ local_ai_model }}
name: text-generation
top_k: 40
top_p: 0.9
temperature: 0.7
repeat_last_n: 64
repeat_penalty: 1.3
path: "{{ local_ai_base_dir }}"
